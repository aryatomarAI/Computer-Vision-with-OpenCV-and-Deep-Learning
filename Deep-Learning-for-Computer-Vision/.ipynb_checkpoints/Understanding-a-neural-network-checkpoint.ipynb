{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Neural Networks\n",
    "\n",
    "Multiple Perceptrons Network is also known as Neural Networks.\n",
    "\n",
    "**Input Layers**\n",
    "\n",
    "Real values from the data.\n",
    "\n",
    "**Hidden Layers**\n",
    "\n",
    "Layers between input and output\n",
    "\n",
    "3 or more layers is \"deep network\"\n",
    "\n",
    "**Output Layers**\n",
    "\n",
    "Final estimate of the output\n",
    "\n",
    "**Several Activation functions that we are going to use---------**\n",
    "\n",
    "* Sigmoid Function (give back output value between 1 and 0)\n",
    "\n",
    "* Hyperbolic Tangent Function (output value between -1 and 1)\n",
    "\n",
    "* Rectified Linear Unit(ReLu)\n",
    "\n",
    "**ReLu and Hyperbolic Tangent function tend to have the best performance, so we will focus on these two.**\n",
    "\n",
    "Deep learning libraries have these built in for us, so we don't need to worry about having to implement them manually.\n",
    "\n",
    "\n",
    "**Cost Function---------**\n",
    "\n",
    "We can use a cost function to measure how far off we are from the expected value.\n",
    "\n",
    "**Best cost functions**\n",
    "\n",
    "* **Quadratic Cost**\n",
    "\n",
    "* **Cross-Entropy**\n",
    "\n",
    "**Gradient Descent----------**\n",
    "\n",
    "* Gradient descent is an optimizaton algorithm for finding minimum of function.\n",
    "\n",
    "* To find a local minimum, we take steps proportional to the negative of the gardient.\n",
    "\n",
    "* Using gradient descent we can figure out the best parameters for minimizing our cost, for example finding the best values for the weights of the neuron inputs.\n",
    "\n",
    "**Back Propagation--------**\n",
    "\n",
    "Back propagation is used to calculate the error contribution of each neuron after a batch of data is processed.\n",
    "\n",
    "It relies heavily on the chain rule to go back through the network and calculate these errors.\n",
    "\n",
    "Back propagation works by calculating the error at the output and then distributes back through the network layers.\n",
    "\n",
    "It requires a known desired output for each input value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
